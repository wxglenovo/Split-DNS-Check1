import os
import msgpack
import requests
import argparse
import dns.resolver
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import hashlib

# ===============================
# é…ç½®åŒºï¼ˆConfigï¼‰
# ===============================
URLS_TXT = "urls.txt"
TMP_DIR = "tmp"
DIST_DIR = "dist"
MASTER_RULE = "merged_rules.txt"
PARTS = 16
DNS_TIMEOUT = 2
DELETE_COUNTER_FILE = os.path.join(DIST_DIR, "delete_counter.bin")
NOT_WRITTEN_FILE = os.path.join(DIST_DIR, "not_written_counter.bin")
RETRY_FILE = os.path.join(DIST_DIR, "retry_rules.txt")
DELETE_THRESHOLD = 4
DNS_BATCH_SIZE = 540
WRITE_COUNTER_MAX = 6
DNS_THREADS = 80
BALANCE_THRESHOLD = 1
BALANCE_MOVE_LIMIT = 50

os.makedirs(TMP_DIR, exist_ok=True)
os.makedirs(DIST_DIR, exist_ok=True)

# ===============================
# æ–‡ä»¶ç¡®ä¿å‡½æ•°ï¼ˆå†™å…¥ç©º msgpack dictï¼‰
# ===============================
def ensure_bin_file(path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    if not os.path.exists(path):
        try:
            with open(path, "wb") as f:
                f.write(msgpack.packb({}, use_bin_type=True))
        except Exception as e:
            print(f"âš  åˆå§‹åŒ– {path} å¤±è´¥: {e}")

ensure_bin_file(DELETE_COUNTER_FILE)
ensure_bin_file(NOT_WRITTEN_FILE)
if not os.path.exists(RETRY_FILE):
    open(RETRY_FILE, "w", encoding="utf-8").close()

# ===============================
# äºŒè¿›åˆ¶è¯»å†™ï¼ˆmsgpackï¼‰
# ===============================
def load_bin(path, print_stats=False):
    if os.path.exists(path):
        try:
            with open(path, "rb") as f:
                raw = f.read()
                if not raw:
                    return {}
                data = msgpack.unpackb(raw, raw=False)
            return data
        except Exception as e:
            print(f"âš  è¯»å– {path} é”™è¯¯: {e}")
            return {}
    return {}

def save_bin(path, data):
    try:
        with open(path, "wb") as f:
            f.write(msgpack.packb(data, use_bin_type=True))      
    except Exception as e:
        print(f"âš  ä¿å­˜ {path} é”™è¯¯: {e}")

# ===============================
# ä¸‹è½½å¹¶åˆå¹¶è§„åˆ™æº
# ===============================
def download_all_sources():
    """
    ä¸‹è½½æ‰€æœ‰è§„åˆ™æºï¼Œåˆå¹¶è§„åˆ™ï¼Œè¿‡æ»¤å¹¶æ›´æ–°åˆ é™¤è®¡æ•°
    """
    if not os.path.exists(URLS_TXT):
        print("âŒ urls.txt ä¸å­˜åœ¨")
        return False
    print("ğŸ“¥ ä¸‹è½½è§„åˆ™æº...")

    merged = set()
    with open(URLS_TXT, "r", encoding="utf-8") as f:
        urls = [u.strip() for u in f if u.strip()]

    for url in urls:
        print(f"ğŸŒ è·å– {url}")
        try:
            r = requests.get(url, timeout=20)
            r.raise_for_status()
            for line in r.text.splitlines():
                line = line.strip()
                if line:
                    merged.add(line)
        except Exception as e:
            print(f"âš  ä¸‹è½½å¤±è´¥ {url}: {e}")

    print(f"âœ… åˆå¹¶ {len(merged)} æ¡è§„åˆ™")

    # ä¿å­˜åˆå¹¶è§„åˆ™åˆ°ä¸´æ—¶æ–‡ä»¶
    temp_file = os.path.join(TMP_DIR, "merged_rules_temp.txt")
    with open(temp_file, "w", encoding="utf-8") as f:
        f.write("\n".join(sorted(merged)))

    return temp_file  # è¿”å›ä¸´æ—¶æ–‡ä»¶çš„è·¯å¾„

# ===============================
# å“ˆå¸Œåˆ†ç‰‡ + è´Ÿè½½å‡è¡¡ä¼˜åŒ–
# ===============================
def split_parts(merged_rules):
    sorted_rules = sorted(merged_rules)
    total = len(sorted_rules)
    part_buckets = [[] for _ in range(PARTS)]
    
    # é¦–å…ˆï¼Œæ ¹æ®è§„åˆ™çš„å“ˆå¸Œå€¼è¿›è¡Œåˆæ­¥åˆ†é…
    for rule in sorted_rules:
        h = int(hashlib.sha256(rule.encode("utf-8")).hexdigest(), 16)
        idx = h % PARTS
        part_buckets[idx].append(rule)

    # ç„¶åï¼Œè¿›è¡Œè´Ÿè½½å‡è¡¡ä¼˜åŒ–
    while True:
        lens = [len(b) for b in part_buckets]
        max_len, min_len = max(lens), min(lens)
        
        # å¦‚æœè´Ÿè½½å·®è·è¶³å¤Ÿå°ï¼Œåˆ™ç»“æŸ
        if max_len - min_len <= BALANCE_THRESHOLD:
            break
        
        max_idx, min_idx = lens.index(max_len), lens.index(min_len)
        move_count = min(BALANCE_MOVE_LIMIT, (max_len - min_len) // 2)
        
        # å¦‚æœç§»åŠ¨æ•°é‡å°äºç­‰äº 0ï¼Œåˆ™é€€å‡º
        if move_count <= 0:
            break
        
        # ä»è´Ÿè½½æœ€å¤§çš„åˆ†ç‰‡ç§»è‡³è´Ÿè½½æœ€å°çš„åˆ†ç‰‡
        part_buckets[min_idx].extend(part_buckets[max_idx][-move_count:])
        part_buckets[max_idx] = part_buckets[max_idx][:-move_count]
    
    # å°†åˆ†é…å¥½çš„è§„åˆ™å†™å…¥æ–‡ä»¶
    for i, bucket in enumerate(part_buckets):
        filename = os.path.join(TMP_DIR, f"part_{i+1:02d}.txt")
        with open(filename, "w", encoding="utf-8") as f:
            f.write("\n".join(bucket))
        print(f"ğŸ“„ åˆ†ç‰‡ {i+1}: {len(bucket)} æ¡è§„åˆ™ â†’ {filename}")

# ===============================
# DNS éªŒè¯
# ===============================
def dns_validate(rules, part):
    valid_rules = []
    total_rules = len(rules)
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç† DNS éªŒè¯
    with ThreadPoolExecutor(max_workers=DNS_THREADS) as executor:
        futures = {executor.submit(check_domain, r): r for r in rules}
        completed, start_time = 0, time.time()
        
        # é€ä¸ªå¤„ç†éªŒè¯ç»“æœ
        for future in as_completed(futures):
            try:
                res = future.result()
                if res:
                    valid_rules.append(res)
            except Exception as e:
                # æ•è·çº¿ç¨‹ä¸­çš„å¼‚å¸¸
                print(f"âš  DNS éªŒè¯å¤±è´¥: {e}")
            completed += 1
            
            # è¾“å‡ºè¿›åº¦ä¿¡æ¯
            if completed % DNS_BATCH_SIZE == 0 or completed == total_rules:
                elapsed = time.time() - start_time
                speed = completed / elapsed if elapsed > 0 else 0
                eta = (total_rules - completed) / speed if speed > 0 else 0
                print(f"âœ… å·²éªŒè¯ {completed}/{total_rules} æ¡ | æœ‰æ•ˆ {len(valid_rules)} æ¡ | é€Ÿåº¦ {speed:.1f}/ç§’ | é¢„è®¡å®Œæˆ {eta:.1f}s")
    
    return valid_rules

# ===============================
# æ›´æ–° not_written_counter
# ===============================
def update_not_written_counter(part_num, validated_rules):
    counter = load_bin(NOT_WRITTEN_FILE)

    # åˆå§‹åŒ–æ¯ä¸ª part çš„è®¡æ•°å™¨
    part_key = f"validated_part_{part_num}"
    counter.setdefault(part_key, {})

    validated_file = os.path.join(DIST_DIR, f"validated_part_{part_num}.txt")

    # ç¡®ä¿æ–‡ä»¶å­˜åœ¨åè¯»å–
    existing_rules = set()
    if os.path.exists(validated_file):
        with open(validated_file, "r", encoding="utf-8") as f:
            existing_rules = set(f.read().splitlines())

    # æ›´æ–°è®¡æ•°å™¨ï¼šå¦‚æœéªŒè¯æˆåŠŸï¼Œè®¾ç½®ä¸º 6ï¼›å¦åˆ™é€’å‡ã€‚
    for rule in validated_rules:
        counter[part_key][rule] = WRITE_COUNTER_MAX  # éªŒè¯æˆåŠŸçš„è§„åˆ™è®¾ç½®ä¸º 6

    for rule in existing_rules - set(validated_rules):
        counter[part_key][rule] = max(counter[part_key].get(rule, WRITE_COUNTER_MAX) - 1, 0)

    # ä¿å­˜æ›´æ–°åçš„è®¡æ•°å™¨
    save_bin(NOT_WRITTEN_FILE, counter)

    # æŸ¥æ‰¾ write_counter <= 0 çš„è§„åˆ™ï¼Œå¹¶å‡†å¤‡é‡è¯•
    to_retry = [r for r in existing_rules if counter[part_key].get(r, 0) <= 0]

    if to_retry:
        # å°†éœ€è¦é‡è¯•çš„è§„åˆ™å†™å…¥ retry_rules.txt æ–‡ä»¶
        with open(RETRY_FILE, "a", encoding="utf-8") as rf:
            rf.write("\n".join(to_retry) + "\n")
        print(f"ğŸ”¥ {len(to_retry)} æ¡ write_counter â‰¤ 0 çš„è§„åˆ™å†™å…¥ {RETRY_FILE}")
        
        # ä»å·²éªŒè¯è§„åˆ™ä¸­åˆ é™¤è¿™äº›é‡è¯•çš„è§„åˆ™
        existing_rules -= set(to_retry)

    # ä¿å­˜æ›´æ–°åçš„ validated_part_X.txt æ–‡ä»¶
    with open(validated_file, "w", encoding="utf-8") as f:
        f.write("\n".join(sorted(existing_rules.union(validated_rules))))

    return len(to_retry)

# ===============================
# å¤„ç†åˆ†ç‰‡
# ===============================
def process_part(part):
    part = int(part)
    part_file = os.path.join(TMP_DIR, f"part_{part:02d}.txt")
    
    # ç¡®ä¿åˆ†ç‰‡æ–‡ä»¶å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œå°è¯•ä¸‹è½½å¹¶æ›´æ–°åˆå¹¶è§„åˆ™
    if not os.path.exists(part_file):
        print(f"âš  åˆ†ç‰‡ {part} ç¼ºå¤±ï¼Œé‡æ–°æ‹‰å–è§„åˆ™â€¦")
        temp_file = download_all_sources()  # è·å–åˆå¹¶è§„åˆ™ä¸´æ—¶æ–‡ä»¶è·¯å¾„
        if not temp_file:
            print("âŒ æ— æ³•è·å–åˆå¹¶è§„åˆ™ï¼Œç»ˆæ­¢")
            return
    else:
        # å¦‚æœåˆ†ç‰‡å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨å·²åŠ è½½çš„åˆå¹¶è§„åˆ™
        temp_file = os.path.join(TMP_DIR, "merged_rules_temp.txt")  # ä½¿ç”¨å·²å­˜åœ¨çš„ä¸´æ—¶æ–‡ä»¶

    if not os.path.exists(part_file):
        print("âŒ åˆ†ç‰‡ä»ä¸å­˜åœ¨ï¼Œç»ˆæ­¢")
        return

    # è¯»å–å¹¶åˆå¹¶é‡è¯•è§„åˆ™å’Œå½“å‰éœ€è¦éªŒè¯çš„è§„åˆ™
    retry_rules = []
    if os.path.exists(RETRY_FILE):
        with open(RETRY_FILE, "r", encoding="utf-8") as rf:
            retry_rules = [l.strip() for l in rf if l.strip()]

    if retry_rules:
        print(f"ğŸ” å°† {len(retry_rules)} æ¡ retry_rules æ’å…¥åˆ†ç‰‡é¡¶éƒ¨å¹¶æ¸…ç©º {RETRY_FILE}")

    combined_rules = retry_rules + [l.strip() for l in open(part_file, "r", encoding="utf-8").read().splitlines()] if retry_rules else [l.strip() for l in open(part_file, "r", encoding="utf-8").read().splitlines()]

    # æ¸…ç©º retry_rules.txt æ–‡ä»¶
    if retry_rules:
        with open(RETRY_FILE, "w", encoding="utf-8") as f:
            f.write("")  # æ¸…ç©ºæ–‡ä»¶

    retry_count = len(retry_rules)  # è®°å½• retry_rules çš„æ•°é‡
    initial_rule_count = len(combined_rules) - retry_count  # æ’å…¥ retry_rules ä¹‹å‰çš„è§„åˆ™æ•°é‡
    total_rules = len(combined_rules)
    print(f"â± åˆ†ç‰‡ {part}: {initial_rule_count} æ¡è§„åˆ™ æ’å…¥{retry_count} æ¡ retry_rules å å…± {total_rules} æ¡è§„åˆ™")

    out_file = os.path.join(DIST_DIR, f"validated_part_{part}.txt")
    old_rules = set(open(out_file, "r", encoding="utf-8").read().splitlines()) if os.path.exists(out_file) else set()

    delete_counter = load_bin(DELETE_COUNTER_FILE)
    rules_to_validate = [r for r in combined_rules if int(delete_counter.get(r, 4)) < 7]

    # åŠ è½½ä¸´æ—¶åˆå¹¶è§„åˆ™
    with open(temp_file, "r", encoding="utf-8") as f:
        merged = set(f.read().splitlines())

    # æ‰§è¡Œ DNS éªŒè¯å¹¶ä¸”å¹¶è¡ŒåŒ–å¤„ç†
    valid = dns_validate(rules_to_validate, part)

    final_rules = set(old_rules)
    added_count = 0
    failure_counts = {}
    discarded_rules = []  # ç”¨æ¥è®°å½•ä¸¢å¼ƒçš„è§„åˆ™
    retry_rules = []  # ç”¨æ¥è®°å½•éœ€è¦é‡è¯•çš„è§„åˆ™

    # å¤„ç†éªŒè¯ç»“æœ
    for r in rules_to_validate:
        write_counter = int(delete_counter.get(r, 0))

        if r in valid:
            final_rules.add(r)
            delete_counter[r] = 6  # å°† write_counter è®¾ç½®ä¸º 6
            added_count += 1
        else:
            # æ›´æ–°å¤±è´¥è®¡æ•°
            delete_counter[r] = write_counter + 1
            fc = min(delete_counter[r], 27)  # åªç»Ÿè®¡ 1/4 è‡³ 27/4 çš„å¤±è´¥è®¡æ•°
            failure_counts[fc] = failure_counts.get(fc, 0) + 1
            if delete_counter[r] >= DELETE_THRESHOLD:
                final_rules.discard(r)
                discarded_rules.append(r)  # è®°å½•ä¸¢å¼ƒçš„è§„åˆ™
                retry_rules.append(r)

    # ä¿å­˜ delete_counter    
    save_bin(DELETE_COUNTER_FILE, delete_counter)

    # æ‰“å°è¿ç»­å¤±è´¥ç»Ÿè®¡ï¼ˆåŒ…æ‹¬ 1/4 è‡³ 7/4ï¼‰
    print("\nğŸ“Š å½“å‰åˆ†ç‰‡è¿ç»­å¤±è´¥ç»Ÿè®¡:")
    for i in range(1, 8):  # æ‰©å±•ç»Ÿè®¡èŒƒå›´ï¼Œæ‰“å° 1/4 è‡³ 7/4
        if failure_counts.get(i, 0) > 0:
            print(f"    âš  è¿ç»­å¤±è´¥ {i}/4 çš„è§„åˆ™æ¡æ•°: {failure_counts[i]}")

    # æ‰“å° write_counter è§„åˆ™ç»Ÿè®¡
    print("ğŸ“Š å½“å‰åˆ†ç‰‡ write_counter è§„åˆ™ç»Ÿè®¡:")
    part_key = f"validated_part_{part}"
    counter = load_bin(NOT_WRITTEN_FILE)
    part_counter = counter.get(part_key, {})

    # åˆå§‹åŒ–æ¯ä¸ª write_counter çš„è®¡æ•°
    counts = {i: 0 for i in range(1, 8)}  # æ”¯æŒ 1 è‡³ 7 çš„ç»Ÿè®¡

    for v in part_counter.values():
        v = int(v)
        if 1 <= v <= 7:  # åªç»Ÿè®¡ 1 è‡³ 7 çš„èŒƒå›´
            counts[v] += 1

    total_rules = sum(counts.values())   
    for i in range(1, 8):
        if counts[i] > 0:
            print(f"    âš  write_counter {i}/4 çš„è§„åˆ™æ¡æ•°: {counts[i]}")

    print("--------------------------------------------------")

    # ä¿å­˜ä¸¢å¼ƒè§„åˆ™åˆ° retry_rules.txt æ–‡ä»¶
    if retry_rules:
        print(f"ğŸ” å†™å…¥ {len(retry_rules)} æ¡ä¸¢å¼ƒè§„åˆ™åˆ° {RETRY_FILE}")
        with open(RETRY_FILE, "a", encoding="utf-8") as f:
            f.write("\n".join(retry_rules) + "\n")
        print(f"ğŸ”¥ {len(retry_rules)} æ¡è§„åˆ™ä¸¢å¼ƒï¼Œå†™å…¥ {RETRY_FILE} ä»¥å¾…é‡è¯•")

    # ä¿å­˜æœ€ç»ˆè§„åˆ™
    print(f"ä¿å­˜æœ€ç»ˆè§„åˆ™åˆ° {out_file}, è§„åˆ™æ•°é‡: {len(final_rules)}")
    with open(out_file, "w", encoding="utf-8") as f:
        f.write("\n".join(sorted(final_rules)))

    # æ›´æ–°æœªå†™å…¥è®¡æ•°å™¨
    deleted_validated = update_not_written_counter(part, valid)
    total_count = len(final_rules)

    print(f"âœ… åˆ†ç‰‡ {part} å®Œæˆ: æ€»{total_count}, æ–°å¢{added_count}, åˆ é™¤{deleted_validated}, è¿‡æ»¤{len(rules_to_validate) - len(valid)}")
    print(f"COMMIT_STATS: æ€» {total_count}, æ–°å¢ {added_count}, åˆ é™¤ {deleted_validated}, è¿‡æ»¤ {len(rules_to_validate) - len(valid)}")

# ===============================
# ä¸»å…¥å£
# ===============================
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--part", help="éªŒè¯æŒ‡å®šåˆ†ç‰‡ 1~16")
    parser.add_argument("--force-update", action="store_true", help="å¼ºåˆ¶é‡æ–°ä¸‹è½½è§„åˆ™æºå¹¶åˆ‡ç‰‡")
    args = parser.parse_args()

    if args.force_update:
        download_all_sources()
    if not os.path.exists(MASTER_RULE) or not os.path.exists(os.path.join(TMP_DIR, "part_01.txt")):
        print("âš  ç¼ºå°‘è§„åˆ™æˆ–åˆ†ç‰‡ï¼Œè‡ªåŠ¨æ‹‰å–")
        download_all_sources()
    if args.part:
        process_part(args.part)
